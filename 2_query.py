# 2_query.py
import os
import sys
from openai import OpenAI
from pymongo import MongoClient
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
DO_GENAI_KEY = os.getenv("DO_GENAI_KEY")
MONGODB_URI = os.getenv("MONGODB_URI")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not DO_GENAI_KEY:
    print("‚ùå Error: DO_GENAI_KEY not found in environment variables")
    sys.exit(1)

if not MONGODB_URI:
    print("‚ùå Error: MONGODB_URI not found in environment variables")
    sys.exit(1)

if not OPENAI_API_KEY:
    print("‚ùå Error: OPENAI_API_KEY not found in environment variables")
    sys.exit(1)

# Initialize clients
try:
    # DigitalOcean client for chat
    do_client = OpenAI(
        api_key=DO_GENAI_KEY,
        base_url="https://inference.do-ai.run/v1"
    )
    
    # OpenAI client for embeddings
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    
    # MongoDB client
    mongo_client = MongoClient(MONGODB_URI)
    db = mongo_client.ai_demo
    collection = db.tickets
    
    print("‚úÖ All services connected successfully!")
    
except Exception as e:
    print(f"‚ùå Connection failed: {e}")
    sys.exit(1)

def generate_embedding(text):
    """Generate embedding using OpenAI API"""
    try:
        response = openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ùå Embedding generation failed: {e}")
        return None

def vector_search(query_text, limit=3):
    """Perform vector search using MongoDB Atlas Vector Search"""
    
    # Generate embedding for query
    query_embedding = generate_embedding(query_text)
    if not query_embedding:
        return []
    
    # Vector search pipeline
    pipeline = [
        {
            "$vectorSearch": {
                "index": "embedding-index",
                "path": "embedding", 
                "queryVector": query_embedding,
                "numCandidates": 50,
                "limit": limit
            }
        },
        {
            "$project": {
                "_id": 0,
                "ticket_id": 1,
                "title": 1,
                "description": 1,
                "category": 1,
                "priority": 1,
                "status": 1,
                "score": {"$meta": "vectorSearchScore"}
            }
        }
    ]
    
    try:
        results = list(collection.aggregate(pipeline))
        return results
    except Exception as e:
        print(f"‚ùå Vector search failed: {e}")
        return []

def generate_rag_response(query, context_docs):
    """Generate RAG response using retrieved context"""
    
    # Build context from retrieved documents
    context = "\n\n".join([
        f"Ticket {doc['ticket_id']}: {doc['title']}\n"
        f"Description: {doc['description']}\n"
        f"Category: {doc['category']}, Priority: {doc['priority']}, Status: {doc['status']}"
        for doc in context_docs
    ])
    
    # Create prompt
    prompt = f"""You are a helpful customer support assistant. Based on the following support ticket information, provide a helpful response to the user's query.

Context (Support Tickets):
{context}

User Query: {query}

Please provide a helpful response based on the relevant tickets above. If the query is about a specific issue, reference similar tickets and suggest solutions."""

    try:
        response = do_client.chat.completions.create(
            model="llama3.3-70b-instruct",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500,
            stream=True
        )
        
        print("ü§ñ AI Response:")
        print("-" * 50)
        
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print("\n" + "-" * 50)
        return full_response
        
    except Exception as e:
        print(f"‚ùå RAG response generation failed: {e}")
        return None

def interactive_query():
    """Interactive query interface"""
    print("\nüîç RAG Query System")
    print("=" * 50)
    print("Enter your queries (type 'quit' to exit)")
    
    while True:
        query = input("\nüí¨ Your question: ").strip()
        
        if query.lower() in ['quit', 'exit', 'q']:
            break
            
        if not query:
            continue
            
        print(f"\nüîÑ Searching for: '{query}'")
        
        # Perform vector search
        results = vector_search(query, limit=3)
        
        if not results:
            print("‚ùå No relevant documents found")
            continue
            
        # Display search results
        print(f"\nüìã Found {len(results)} relevant tickets:")
        for i, doc in enumerate(results, 1):
            print(f"\n{i}. Ticket {doc['ticket_id']}: {doc['title']}")
            print(f"   Category: {doc['category']} | Priority: {doc['priority']} | Status: {doc['status']}")
            print(f"   Similarity Score: {doc['score']:.4f}")
            print(f"   Description: {doc['description'][:100]}...")
        
        # Generate RAG response
        print(f"\nü§ñ Generating response...")
        rag_response = generate_rag_response(query, results)

if __name__ == "__main__":
    # Check if data exists
    doc_count = collection.count_documents({})
    if doc_count == 0:
        print("‚ùå No documents found in database")
        print("Please run 'python 1_ingest.py' first to ingest data")
        sys.exit(1)
    
    print(f"üìä Found {doc_count} documents in database")
    
    # Start interactive query
    interactive_query()
    
    # Close connections
    mongo_client.close()
    print("\nüîí Database connection closed") 